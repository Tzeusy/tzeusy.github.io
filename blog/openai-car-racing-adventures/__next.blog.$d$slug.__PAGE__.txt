1:"$Sreact.fragment"
2:I[5500,["/_next/static/chunks/1b2d64756f20754f.js"],"Image"]
28:I[22016,["/_next/static/chunks/1b2d64756f20754f.js"],""]
29:I[97367,["/_next/static/chunks/ff1a16fafef87110.js","/_next/static/chunks/d2be314c3ece3fbe.js"],"OutletBoundary"]
2a:"$Sreact.suspense"
0:{"buildId":"y6JfkF5vJlj65YhSLu4Tc","rsc":["$","$1","c",{"children":[["$","article",null,{"className":"space-y-10","children":[null,["$","header",null,{"className":"space-y-4","children":[["$","p",null,{"className":"text-sm text-muted","children":[["$","time",null,{"dateTime":"2020-03-23","children":"2020-03-23"}]," · ","6 min read"]}],["$","h1",null,{"className":"text-balance text-4xl sm:text-5xl","children":"OpenAI Car Racing Adventures"}],["$","p",null,{"className":"text-lg text-muted","children":"An overview of our Imitation Learning approach towards creating a model to play the OpenAI-CarRacingv0 environment."}],["$","p",null,{"className":"text-sm text-muted","children":[["$","span","machine-learning",{"children":["","machine-learning"]}],["$","span","reinforcement-learning",{"children":[" · ","reinforcement-learning"]}],["$","span","imitation-learning",{"children":[" · ","imitation-learning"]}],["$","span","openai-gym",{"children":[" · ","openai-gym"]}],["$","span","deep-learning",{"children":[" · ","deep-learning"]}]]}]]}],["$","nav",null,{"className":"space-y-2 border-y border-rule py-4","children":[["$","p",null,{"className":"text-sm uppercase tracking-[0.08em] text-accent","children":"In this essay"}],["$","ol",null,{"className":"space-y-1 text-sm text-muted","children":[["$","li","introduction",{"style":{"paddingLeft":"0rem"},"children":["$","a",null,{"href":"#introduction","children":"Introduction"}]}],["$","li","problem-scope",{"style":{"paddingLeft":"0rem"},"children":["$","a",null,{"href":"#problem-scope","children":"Problem Scope"}]}],["$","li","neural-network-architecture",{"style":{"paddingLeft":"0rem"},"children":["$","a",null,{"href":"#neural-network-architecture","children":"Neural Network architecture"}]}],["$","li","approaches",{"style":{"paddingLeft":"0rem"},"children":["$","a",null,{"href":"#approaches","children":"Approaches"}]}],["$","li","double-deep-q-learning",{"style":{"paddingLeft":"1rem"},"children":["$","a",null,{"href":"#double-deep-q-learning","children":"Double Deep Q Learning"}]}],["$","li","reinforce-with-baseline",{"style":{"paddingLeft":"1rem"},"children":["$","a",null,{"href":"#reinforce-with-baseline","children":"REINFORCE with Baseline"}]}],["$","li","advantage-actor-critic",{"style":{"paddingLeft":"1rem"},"children":["$","a",null,{"href":"#advantage-actor-critic","children":"Advantage Actor-Critic"}]}],["$","li","imitation-learning",{"style":{"paddingLeft":"1rem"},"children":["$","a",null,{"href":"#imitation-learning","children":"Imitation Learning"}]}],["$","li","results",{"style":{"paddingLeft":"0rem"},"children":["$","a",null,{"href":"#results","children":"Results"}]}],["$","li","conclusion",{"style":{"paddingLeft":"0rem"},"children":["$","a",null,{"href":"#conclusion","children":"Conclusion"}]}]]}]]}],["$","section",null,{"className":"blog-post-content-underlines space-y-4 text-justify hyphens-auto","children":[["$","h1",null,{"className":"mt-10 text-4xl sm:text-5xl ","id":"introduction","children":"Introduction"}],"\n",["$","p",null,{"className":"my-5 text-base text-foreground ","children":"The OpenAI Car Racing gym environment is a continuous control image-based task, whereby a car is trained to navigate a randomly generated racetrack. This post describes the team's efforts in investigating differing approaches to creating an effective generalizable model, and an interactive GUI for user interpretation of model and parameter efficacy. Approaches such as Double Deep Q Learning, Policy Gradient, Advantage Actor-Critic (A2C), and Imitation Learning are taken with accompanying results analyzed, and a system built for contrasting efficacy of models allows human players to observe in real-time the impact of parameter changes or imitation-based suggested movements."}],"\n",["$","h1",null,{"className":"mt-10 text-4xl sm:text-5xl ","id":"problem-scope","children":"Problem Scope"}],"\n",["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","$L2",null,{"className":"my-6 h-auto w-full rounded-md border border-rule ","alt":"Introduction","src":"/blog/openai-car-racing-adventures/environment-overview.png","width":1200,"height":630,"sizes":"100vw"}]}],"\n",["$","p",null,{"className":"my-5 text-base text-foreground ","children":["The ","$L3"," takes in continuous input, with a 96x96 pixel state and a time-to-completion based reward scheme. New racing tracks are generated every iteration - these tracks include turns of varying sharpness, and will always loop back to their starting point in a counter-clockwise manner. The environment provides a -0.1 reward per frame and +1000/N for every track tile visited, where N is the total number of tiles in the track, imposing an upper bound of 1000 for possible rewards. The controls for the car are Left, Right, Accelerate, and Brake, which are non-exclusive. Additional indicators at the bottom of the screen provided include true speed, four ABS sensors, steering wheel position, and gyroscope."]}],"\n","$L4","\n","$L5","\n","$L6","\n","$L7","\n","$L8","\n","$L9","\n","$La","\n","$Lb","\n","$Lc","\n","$Ld","\n","$Le","\n","$Lf","\n","$L10","\n","$L11","\n","$L12","\n","$L13","\n","$L14","\n","$L15","\n","$L16","\n","$L17","\n","$L18","\n","$L19","\n","$L1a","\n","$L1b","\n","$L1c","\n","$L1d","\n","$L1e","\n","$L1f","\n","$L20","\n","$L21","\n","$L22","\n","$L23","\n","$L24","\n","$L25"]}],null,"$L26"]}],null,"$L27"]}],"loading":null,"isPartial":false}
3:["$","a",null,{"href":"https://gymnasium.farama.org/environments/box2d/car_racing/","children":"OpenAI Car Racing Gym environment"}]
4:["$","h1",null,{"className":"mt-10 text-4xl sm:text-5xl ","id":"neural-network-architecture","children":"Neural Network architecture"}]
5:["$","div",null,{"style":{"maxWidth":"30%"},"children":["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","$L2",null,{"className":"my-6 h-auto w-full rounded-md border border-rule ","alt":"Neural network architecture","src":"/blog/openai-car-racing-adventures/neural-network-architecture.png","width":1200,"height":630,"sizes":"100vw"}]}]}]
6:["$","p",null,{"className":"my-5 text-base text-foreground ","children":"As the goal was to test our various Reinforcement learning approaches, we decided to standardize a convolutional neural network with fully connected layers with five output neurons, which are softmaxed to yield exclusive categorical values, representing the decisions (e.g. accelerate, left, right). This mimics human behaviour, with the notable exception of disallowing multiple simultaneous commands (e.g. ACCELERATE and LEFT at the same time). This is due to the nature of the gym environment designed - simultaneous pressing of keys has a very high likelihood of causing the vehicle to skid out of control."}]
7:["$","h1",null,{"className":"mt-10 text-4xl sm:text-5xl ","id":"approaches","children":"Approaches"}]
8:["$","p",null,{"className":"my-5 text-base text-foreground ","children":"We implemented several Reinforcement Learning models in our initial exploration for solutions to the problem. They are described as follows:"}]
9:["$","h2",null,{"className":"mt-10 text-3xl ","id":"double-deep-q-learning","children":"Double Deep Q Learning"}]
a:["$","p",null,{"className":"my-5 text-base text-foreground ","children":"The Q learning algorithm was our first approach as an initial exploration of the feasibility of solving the racing car problem. We implemented Double Deep Q Learning, notably to avoid the problem of overestimation caused by traditional Q Learning. Double Q Learning uses two Q-value estimators, using each to update the other. These independent estimators hence each reduce overestimation, avoiding the maximization bias of using only one Q value estimator."}]
b:["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","$L2",null,{"className":"my-6 h-auto w-full rounded-md border border-rule ","alt":"Double Deep Q Learning","src":"/blog/openai-car-racing-adventures/double-deep-q-learning.png","width":1200,"height":630,"sizes":"100vw"}]}]
c:["$","h2",null,{"className":"mt-10 text-3xl ","id":"reinforce-with-baseline","children":"REINFORCE with Baseline"}]
d:["$","p",null,{"className":"my-5 text-base text-foreground ","children":"REINFORCE with Baseline was our second model of choice since it gives a n-step roll out estimation of V and hence, should make our training more stable as compared to Double Q Learning. Moreover, REINFORCE with Baseline tends to learn faster than REINFORCE alone."}]
e:["$","h2",null,{"className":"mt-10 text-3xl ","id":"advantage-actor-critic","children":"Advantage Actor-Critic"}]
f:["$","p",null,{"className":"my-5 text-base text-foreground ","children":["We also chose to try out A2C as A2C tends to be more stable than Double Q Learning. On top of that, A2C updates the model more frequently than REINFORCE. A2C rolls an episode out for ",["$","em",null,{"children":"t"}]," max steps, updates the model, then continues again. We were hoping that by doing so, our model would be able to learn more quickly."]}]
10:["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","$L2",null,{"className":"my-6 h-auto w-full rounded-md border border-rule ","alt":"Advantage Actor-Critic","src":"/blog/openai-car-racing-adventures/advantage-actor-critic.png","width":1200,"height":630,"sizes":"100vw"}]}]
11:["$","h2",null,{"className":"mt-10 text-3xl ","id":"imitation-learning","children":"Imitation Learning"}]
12:["$","p",null,{"className":"my-5 text-base text-foreground ","children":"Taking inspiration from alternative approaches towards solving reinforcement learning problems, we decided to implement Imitation Learning as a possible way of developing an effective car racing model, as previous models yielded poor results after weeks of training. The Imitation Learning algorithm uses an underlying Double Deep Q Learning approach, but introduces an additional element via a suggested action involved. We suggest an implementation which penalizes the model reward attained if the choice taken is dissimilar to that suggested by the user. This penalty comes in the form of a modification to the reward initially yielded by the environment via direct addition or subtraction (e.g. a value of 5). This suggested implementation is as follows:"}]
13:["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","$L2",null,{"className":"my-6 h-auto w-full rounded-md border border-rule ","alt":"Imitation Learning","src":"/blog/openai-car-racing-adventures/imitation-learning.png","width":1200,"height":630,"sizes":"100vw"}]}]
14:["$","h1",null,{"className":"mt-10 text-4xl sm:text-5xl ","id":"results","children":"Results"}]
15:["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","$L2",null,{"className":"my-6 h-auto w-full rounded-md border border-rule ","alt":"REINFORCE performance","src":"/blog/openai-car-racing-adventures/reinforce-baseline-performance.png","width":1200,"height":630,"sizes":"100vw"}]}]
16:["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","em",null,{"children":"REINFORCE with Baseline: Peak mean-100 score of -200 (consistently fatal accidents)"}]}]
17:["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","$L2",null,{"className":"my-6 h-auto w-full rounded-md border border-rule ","alt":"Advantage Actor-Critic Performance","src":"/blog/openai-car-racing-adventures/a2c-performance.png","width":1200,"height":630,"sizes":"100vw"}]}]
18:["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","em",null,{"children":"Advantage Actor-Critic: Peak mean-100 score of -50 (One successful turn, or handles long initial stretch well)"}]}]
19:["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","$L2",null,{"className":"my-6 h-auto w-full rounded-md border border-rule ","alt":"DDQN with RMSProp optimizer performance","src":"/blog/openai-car-racing-adventures/ddqn-rmsprop-performance.png","width":1200,"height":630,"sizes":"100vw"}]}]
1a:["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","em",null,{"children":"DDQN with RMSProp optimizer: Peak mean-100 score of 47."}]}]
1b:["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","$L2",null,{"className":"my-6 h-auto w-full rounded-md border border-rule ","alt":"DDQN with ADAM optimizer performance","src":"/blog/openai-car-racing-adventures/ddqn-adam-performance.png","width":1200,"height":630,"sizes":"100vw"}]}]
1c:["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","em",null,{"children":"DDQN with ADAM optimizer: Peak mean-100 score of 300, with occasional successful completions of the track (generally, scores >700 are successful runs)"}]}]
1d:["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","$L2",null,{"className":"my-6 h-auto w-full rounded-md border border-rule ","alt":"DDQN with Imitation Learning parameters performance","src":"/blog/openai-car-racing-adventures/ddqn-imitation-learning-performance.png","width":1200,"height":630,"sizes":"100vw"}]}]
1e:["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","em",null,{"children":"DDQN with Imitation Learning parameters: Peak mean-100 score of 620, with far more frequent completions of the track, though still not consistent. Note the vastly reduced number of training episodes needed."}]}]
1f:["$","p",null,{"className":"my-5 text-base text-foreground ","children":"Of unsupervised Reinforcement Learning algorithms, the best found was Double-Deep Q Learning with the ADAM optimizer, which managed to yield a mean-100 score of 300, among several successful completions of the track, after about 16 hours of training."}]
20:["$","p",null,{"className":"my-5 text-base text-foreground ","children":"Upon subsequent exploration of feasible model generation, we found that imitation learning was an effective way of bootstrapping an initial model, achieving the same mean-100 score of 300 within thirty episodes of manual training. The use of model visualization tools such as Layer Wise Relevance Propagation also allowed us to visualize the effect of training on our models, highlighting notable landmarks on the racetrack and showing subsequent model behaviour. This, along with the developed GUI tools, will allow users to easily tweak hyperparameters and model setup to effectively tune for better generalization and results."}]
21:["$","p",null,{"className":"my-5 text-base text-foreground ","children":"With our implementation of Imitation Learning, we were surprised at how quickly the model improved with the presence of human input. We hence decided to craft the user interfacing platform to show this phenomenon, allowing users to set hyperparameters before engaging in this imitation learning system. The intended user flow is for the user to input suggested actions for every frame of the episode, watching the user-guided model rapidly learn how to navigate on the racing track. Developments in the LRP can also be observed, showing the gradual appearance of notable features within the CNN over episodes. We observe that with this approach, the model is able to successfully make basic turns after only 10 episodes (about 5 minutes) of training, and able to complete the track after 20 to 25 (depending on the users). This is in strict contrast to the training time needed for other methods, such as DDQN, which required over 1400 episodes (about 12 hours) before the first successful completed run of the track."}]
22:["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","$L2",null,{"className":"my-6 h-auto w-full rounded-md border border-rule ","alt":"Layer Wise Relevance Propagation Visualization","src":"/blog/openai-car-racing-adventures/lrp-visualization.gif","width":1200,"height":630,"sizes":"100vw"}]}]
23:["$","p",null,{"className":"my-5 text-base text-foreground ","children":["$","em",null,{"children":"Significant regions of the road are clearly highlighted in the LRP layer, applied to visualize the neural network's regions of significance"}]}]
24:["$","h1",null,{"className":"mt-10 text-4xl sm:text-5xl ","id":"conclusion","children":"Conclusion"}]
25:["$","p",null,{"className":"my-5 text-base text-foreground ","children":"Imitation Learning proved to be the most effective approach for this environment, achieving a peak mean-100 score of 620 and reducing training time from over 12 hours (with DDQN) to roughly 5 minutes of human-guided episodes. The combination of human-suggested actions with Double Deep Q Learning allowed the model to bootstrap far more efficiently than any purely unsupervised method we tested. The accompanying GUI and LRP visualization tools make it straightforward to observe how the model develops its understanding of the track over time."}]
26:["$","footer",null,{"className":"grid gap-4 border-t border-rule pt-8 text-sm sm:grid-cols-2","children":[["$","div",null,{"children":[["$","p",null,{"className":"text-muted","children":"Previous"}],["$","$L28",null,{"href":"/blog/setting-up-of-i3wm-ricing","children":"Setting up of i3wm [Ricing]"}]]}],["$","div",null,{"className":"sm:text-right","children":[["$","p",null,{"className":"text-muted","children":"Next"}],["$","$L28",null,{"href":"/blog/market-databases","children":"The Fed and the Coronavirus"}]]}]]}]
27:["$","$L29",null,{"children":["$","$2a",null,{"name":"Next.MetadataOutlet","children":"$@2b"}]}]
2b:null
